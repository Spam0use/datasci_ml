Weight lifting technique classification
========================================================
```{r echo=FALSE}
library(caret)
library(doParallel)
registerDoParallel(cores=4)
```

## Data acquisition
Load data from csv file, elimate row numbers and timestamps, confirm features are the same
```{r cache=TRUE}
dattr=read.csv('pml-training.csv')[,-c(1,3:5)]  #eliminate row numbers & timestamps
dattst=read.csv('pml-testing.csv')[,-c(1,3:5)] 
all.equal(colnames(dattst)[-ncol(dattst)],colnames(dattr)[-ncol(dattr)])
```

set aside validation set:

```{r cache=TRUE}
library(caret)
set.seed(123)
tridx=createDataPartition(dattr$classe,p=.8,list=FALSE)
tr=dattr[tridx,]
val=dattr[-tridx,]
```


eliminate near zero variance predictors:

```{r cache=TRUE}
nzv=nearZeroVar(tr)
tr=tr[,-nzv]
val=val[,-nzv]
dattst=dattst[,-nzv]
```

eliminate highly correlated predictors:

```{r cache=TRUE}
numcols=which(sapply(tr,is.numeric))  #numeric columns
corcols=findCorrelation(cor(tr[,numcols],use='pairwise'),cutoff=.95)  #columns with correlation > .95 with another column
rmcor=numcols[corcols]  #indices of columns to remove
tr=tr[,-rmcor]
val=val[,-rmcor]
dattst=dattst[,-rmcor]
numcols=which(sapply(tr,is.numeric))  #numeric columns 

```

At this point, 85 predictors remain, but 34 of them contain missing values (and 98% of rows contain a missing value).  Is it reasonable to impute these missing values?  Try building models with imputation and with exclusion of columns containing missing values & compare performance on validation set.

```{r cache=TRUE}
trc=trainControl(method='cv',number=5,selectionFunction='oneSE')  #5-fold xval, bias toward 'smaller' models
nona=apply(is.na(tr),2,any)
trnn=tr[,!nona]
valnn=tr[,!nona]
rfnn=train(trnn[,-ncol(trnn)],trnn$classe,'rf',tuneGrid=data.frame(.mtry=c(6,12,24)),trControl=trc,ntree=200) #use somewhat slimmed down training to speed things up
```
```{r cache=TRUE}
pp=preProcess(tr[,numcols],method='knnImpute')
trimp=tr
trimp[,numcols]=predict(pp,tr[,numcols])
valimp=val
valimp[,numcols]=predict(pp,val[,numcols])
rfimp=train(trimp[,-ncol(trimp)],trimp$classe,'rf',tuneGrid=data.frame(.mtry=c(10,20,30)),trainControl=trc,ntree=200)

```

```{r}
confusionMatrix(predict(rfimp,valimp[,-ncol(valimp)]),valimp[,ncol(valimp)])
confusionMatrix(predict(rfnn,valnn[,-ncol(valnn)]),valnn[,ncol(valnn)])
```

Validation set error is lower for nonimputed data (i.e. using only features with nonmissing data).  Therefore use nonimputed model for test set predictions.  

## Final model
Retrain no-missing-value model using all training data with appropriately adjusted training parameters:

```{r cache=TRUE}
trc=trainControl(method='cv',number=10,selectionFunction='oneSE')  #10-fold xval, bias toward 'smaller' models
dattrnn=rbind(trnn,valnn)
rfnnall=train(dattrnn[,-ncol(dattrnn)],dattrnn$classe,'rf',tuneGrid=data.frame(.mtry=c(3,6,12)),trControl=trc)
```

predictions for test set:
```{r}
dattstnn=dattst[,colnames(dattrnn)[-ncol(dattrnn)]]
answers=predict(rfnnall,dattstnn)
answers

pml_write_files = function(x){  #output function
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(as.character(answers))
```


